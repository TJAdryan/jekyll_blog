[
  
  {
    "title": "Taking the Small Wins: How Incremental Improvements Cut My API Call Times from 70 to 45 Minutes",
    "url": "/python/performance/apis/dataengineering/orjson/optimization/2025/07/19/Taking-the-Small-Wins.html",
    "categories": "Python, Performance, APIs, DataEngineering, orjson, Optimization",
    "tags": "Python, API, performance, optimization, json, orjson, benchmark, data pipeline, incremental improvement, long-term code health",
    "date": "2025-07-19 12:00:00 -0400",
    "content": "As someone who looks back at their own code and sees a mix of pride and regret, I am often torn between the desire to improve what is there and the fear of breaking something that already works. I am sure this is a common feeling, and not just something I suffer from (right?). However, sometimes there are opportunities to make small, incremental improvements that can lead to significant performance gains without with just a small risk of a catastrophic failure. This post is about one such improvement I made recently that cut my nightly API call times from 70 minutes to 45 minutes—a 36.44% speedup—by optimizing how I parsed JSON data.  The Nightly Grind: My API Performance Bottleneck  One of the processes our team relies on is a nightly API call. Its purpose: to fetch the current status for approximately 3,000 distinct objects. This process had a frustratingly consistent completion time of 70 to 80 minutes. The process runs overnight, so while it wasn’t critical to reduce the time, it still presented an increased risk of failure due to the long duration. If something went wrong, it would hold back other processes, and the accumulation would mean the longer it took to discover the issue, the longer it would take to recover.  The complexity stemmed not only from the sheer volume of calls but also from the nature of the API responses. The JSON data returned for each object was deeply nested, and crucially, the specific information I needed wasn’t consistently located under the same header. My parsing logic often involved a series of attempts: try Header A, if not found, try Header B, and so on. This added considerable overhead to each object’s processing.  My initial attempts to speed things up focused on concurrency. I pushed Python’s ThreadPoolExecutor as far as I could, making multiple requests simultaneously. This certainly helped, but I quickly slammed into the API’s inherent rate limits, which effectively capped how much more parallelization I could achieve.  In a search for further optimization, I even experimented with downloading all the raw JSON data first, for all 3,000 objects, and then parsing it in a separate, dedicated step. This was a revelation for raw data acquisition: the data was downloaded in under 15 minutes! However, the next step of parsing and restructuring that massive, inconsistent raw JSON proved to be incredibly cumbersome and complex. The code became brittle, and I found I was having to retool all the parsing logic. I realized I was running into a wall.  I needed a more fundamental change to how I was handling JSON parsing, which turned out to be the place where I could get a significant performance boost without overhauling the entire process.  The Root of the Problem: Slow JSON Parsing  When your Python application receives data from an API, it arrives as a JSON string or byte stream. This raw data must be converted (“parsed” or “deserialized”) into Python objects (like dictionaries and lists) before you can work with it.  Python’s built-in json module, while reliable and universally available, performs this parsing primarily in Python. For a high volume of calls or large, complex JSON payloads, this Python-level execution can introduce significant overhead. It becomes a CPU-bound operation that can limit your application’s overall throughput, even if your network requests are efficient.  orjson: The Performance Enhancer  orjson is a JSON library for Python implemented in Rust. By leveraging Rust’s compiled, highly optimized nature, orjson can perform JSON parsing and serialization operations dramatically faster than the built-in json module. It bypasses many of Python’s inherent runtime overheads, directly boosting processing speed.  Key characteristics of orjson:     Exceptional Speed: Benchmarks consistently show orjson deserialization (loads) outperforming json.loads() by significant margins.   JSON Standard Compliance: It strictly adheres to the JSON specification (RFC 8259) and UTF-8 encoding.   Native Type Support: It intelligently handles common Python types such as datetime, UUID, and numpy arrays directly, reducing the need for custom serializers/deserializers.   Familiar API: Its loads and dumps functions closely mirror the standard json module’s interface, making it relatively straightforward to integrate into existing code. (Note: orjson.dumps returns bytes, while json.dumps returns str).   Quantifying the Gain: A Practical Benchmark  To demonstrate orjson’s impact, let’s run a controlled benchmark. We’ll generate a large, representative JSON dataset and measure the parsing time for both json and orjson over multiple repetitions using the timeit module for accuracy.  Installation:  pip install orjson   Benchmark Script:  import json import orjson import timeit import datetime import uuid import sys  # Function to generate a large, nested JSON structure for testing def create_large_test_json(num_entries=10000): # Using 10,000 entries to match your previous output     entries = []     for i in range(num_entries):         current_entry = {             \"id\": f\"entry-{i}-{uuid.uuid4()}\", # Include UUID for more realistic complexity             \"name\": f\"Item Name {i}\",             \"value\": i * 1.23,             \"details\": {                 \"category\": \"Category \" + str(i % 10),                 \"status\": \"Online\" if i % 2 == 0 else \"Offline\",                 \"tags\": [\"tag_a\", \"tag_b\", \"tag_c\"] if i % 2 == 0 else [\"tag_x\", \"tag_y\"],                 \"created_at\": datetime.datetime.now(datetime.timezone.utc).isoformat() # Include datetime             },             \"config\": {\"ip\": f\"192.168.1.{100 + i}\", \"os\": \"Linux\"},             \"history\": [{\"timestamp\": timeit.default_timer() - j, \"event\": f\"Event {j}\"} for j in range(5)]         }         entries.append(current_entry)      # Use orjson.dumps for efficient generation of the test data     return orjson.dumps({\"data\": entries})  # Generate test data based on your scale TEST_DATA_BYTES = create_large_test_json(num_entries=10000) print(f\"Generated test data size: {len(TEST_DATA_BYTES) / (1024*1024):.2f} MB\")  # Number of times to repeat the parsing operation for more accurate timing NUM_REPETITIONS = 100  print(f\"\\nRunning benchmarks with {NUM_REPETITIONS} repetitions...\")  # Benchmark json.loads() time_json = timeit.timeit(lambda: json.loads(TEST_DATA_BYTES), number=NUM_REPETITIONS) print(f\"Built-in json.loads() took: {time_json:.6f} seconds (total for {NUM_REPETITIONS} runs)\")  # Benchmark orjson.loads() time_orjson = timeit.timeit(lambda: orjson.loads(TEST_DATA_BYTES), number=NUM_REPETITIONS) print(f\"orjson.loads() took:      {time_orjson:.6f} seconds (total for {NUM_REPETITIONS} runs)\")  # Confirm parsing accuracy (performed once for correctness) parsed_data_json_single = json.loads(TEST_DATA_BYTES) parsed_data_orjson_single = orjson.loads(TEST_DATA_BYTES)  if parsed_data_json_single == parsed_data_orjson_single:     print(\"\\nParsing results are identical for a single run.\") else:     print(\"\\nWARNING: Parsing results differ for a single run.\")  # Calculate speedup ratio if time_orjson &gt; 0:     speedup_factor = time_json / time_orjson     print(f\"\\norjson is {speedup_factor:.2f}x faster than built-in json for deserialization (over {NUM_REPETITIONS} runs).\") else:     print(\"orjson was extremely fast, time close to zero.\")   Interpretation of Results:  Using the benchmark with 10,000 entries and 100 repetitions, we obtained the following results, consistent with real-world scenarios:     Generated test data size: 4.31 MB   Built-in json.loads() took: 25.002522 seconds (total for 100 runs)   orjson.loads() took: 15.892222 seconds (total for 100 runs)   These results demonstrate that orjson is approximately 36.44% faster than the built-in json module for deserialization in this benchmark.  To translate this directly to my problem: if JSON parsing was a significant portion of my 70-minute nightly API call, a 36.44% speedup in parsing time could translate into substantial real-world savings. For instance, if a process relying heavily on JSON parsing currently takes 1 hour and 10 minutes (70 minutes), a 36.44% improvement could reduce that specific processing step to approximately 45 minutes. This 25-minute reduction is a tangible win, directly impacting how quickly my data pipelines complete.  Integration with API Workflows  Integrating orjson into your existing API client setup (e.g., using the requests library) is straightforward:     Import orjson: import orjson   Use orjson.loads() on response.content: Instead of response.json(), which internally uses json.loads(), explicitly call orjson.loads(response.content). response.content provides the raw byte data, which orjson prefers for optimal performance.   import requests import orjson import time  api_url = \"[https://jsonplaceholder.typicode.com/posts/1](https://jsonplaceholder.typicode.com/posts/1)\" # Example API endpoint  try:     response = requests.get(api_url, timeout=10)     response.raise_for_status()       parse_start = time.perf_counter()     api_data = orjson.loads(response.content)     parse_end = time.perf_counter()          print(f\"API call successful. JSON parsing took: {parse_end - parse_start:.6f} seconds.\")     # Process api_data (now a Python dictionary)     print(f\"Parsed data: {api_data.get('title')[:30]}...\")  except requests.exceptions.RequestException as e:     print(f\"API request failed: {e}\") except orjson.JSONDecodeError as e:     print(f\"JSON parsing failed: {e}\")   Conclusion  Optimizing JSON parsing is a practical and effective method to enhance API interaction speed, especially when dealing with high data volumes or complex structures. As my own experience demonstrated, identifying and addressing bottlenecks like slow parsing with tools like orjson can yield substantial performance gains. And that’s the real lesson here, isn’t it? It’s not always about needing a massive rewrite. Sometimes, those small, steady improvements—saving a few minutes here and there—are what truly make your code better and your daily work smoother in the long run. Just remember to back up your code before you start tinkering. Your future self might not send a thank-you card, but you will defintely appreciate that you took the time to do take the small wins.  On a serious note, none of this would be possible without the hard work of the orjson team. If you haven’t already, I highly recommend checking out their GitHub repository. It might sound hokey, but I can’t believe how much my life has been improved by people I have or probably will never meet. "
  },
  
  {
    "title": "Building Data Pipelines with Dagster: Practical",
    "url": "/data%20engineering/python/orchestration/2025/06/29/Better-Pipelines-with-Dagster.html",
    "categories": "Data Engineering, Python, Orchestration",
    "tags": "Dagster, data pipelines, Python, orchestration, data engineering",
    "date": "2025-06-29 00:00:00 -0400",
    "content": "I am always looking for ways to improve my data pipelines, and Dagster has been really good to me so far.  I am definitely planning on diving further into it in the future but my initial experiences have been favorable.  Of course, I ran into a a few issues.  But mostly I was able to get things up and running, without too much trouble, and get value from using it right away.  I tried to keep it simple, flowing from the NYC Open Data API through various Dagster assets, eventually feeding into both static PNG charts and the dynamic Streamlit dashboard.  Dagster updates the information and then it is displayed in the Streamlit app.  Here is an exmple of what the output looks like:    The output from Dagster is no slouch when it comes to visualizations either.  Being able to see the orchestration visually and being able to choose to run a single asset is great.    Also the way Dagster displays the logs and allows you to trace back any issues is super helpful too.  Once you start a job you can move over to the runs tab, or you can follow the link that gets generated when you run a job.   If you are interested in getting some ideas for yourself you can follow the README which you to check out directly on the GitHub repo and walks through the process step by step.  It was great to be able to use Dagster’s built-in decorators to define assets and jobs, making the code cleaner and more maintainable. The use of @asset for defining assets and @job for was helpful for organizing the jobs.  I did get tripped up from time to time, but the Dagster documentation was really helpful.  I also found the Dagster Slack community had a lot of great hints and encouraging advice. Dagster examples tend to be dedicated to helping people with current workload issues especially in the sciences. I love this.  The scheuduling from inside Dagster is also really nice.  As well as the logs and the ability to find the origins of errors relatively quickly.  I would love to see how other people are using Dagster in their projects.  If you have any tips or tricks, please let me know.  "
  },
  
  {
    "title": "Bring me the Data",
    "url": "/data%20engineering/python/api/2025/06/14/Bring-me-the-Data.html",
    "categories": "Data Engineering, Python, API",
    "tags": "API, Pandas, data analysis, Python, testing",
    "date": "2025-06-14 12:00:00 -0400",
    "content": "  taking it personally.  NYC Restaurant Inspections Data Pull  When I was making the DuckDB posts, I wanted to include some code to show how to pull the data from the NYC Open Data portal. In the interest of keeping it simple, I just showed how to download the data.  Here’s a separate post that provides a simple way to pull the data programmatically, if you haven’t accessed the NYC Open Data portal before, this is a great way to get started and can easily be adpated to the many other datasets available there.  It is great for practice and to help develop your skills to automate a small call everyday.  To further challenge yourself, you can see how try to see how many days in a row you can pull the data, or to script out jobs to find if there are missing days in your code.  I would say if you come accross an issue and you are not sure what to do, it is probably an issue people who work in data engineering face too.  There is a lot of help out and reccomendations of best policy, but no one has all the answers.  So be humble, but by no means be humbled.  This post is available as code on GitHub and is adapted from the readme.  Getting Started  Follow these steps to set up and run the data pull script.  Prerequisites  Before you begin, ensure you have the following installed:     Python 3.x   pip (Python package installer)   Installation  Clone the repository (or download the files directly):  git clone https://github.com/TJAdryan/nyc-restaurant-inspections.git cd nyc-restaurant-inspections   Install the required Python libraries:  pip install -r requirements.txt   Setting up Your Environment (and Securing Your Token!)  It’s good practice to keep your API tokens out of your main code files and never commit them to version control. This project uses environment variables for secure token management.  Get an NYC Open Data App Token:  While many NYC Open Data endpoints can be accessed without a token for basic queries, having one provides higher rate limits and ensures consistent access. You can get one for free by signing up on the NYC Open Data portal.  Create a .env file:  In the root directory of your cloned repository, create a file named .env. This file will store your API token. You can use the provided .env.example as a template.  Add your app token to .env:  Open the .env file and add your token like so:  MY_APP_SEC=\"YOUR_APP_TOKEN_GOES_HERE\"   Replace “YOUR_APP_SEC_GOES_HERE” with the actual token you obtained.  Running the Script  Once you have installed the dependencies and set up your .env file, you can run the data pull script:  python pull_data.py   The script will print progress messages to your console, and once complete, it will save the retrieved restaurant inspection data as CSV and Parquet files in the same directory.  Code Overview  The pull_data.py script performs the following key actions:     Configuration: Sets up the NYC Open Data endpoint for restaurant inspections and defines the date range.   Date Range Calculation: Dynamically calculates a date range that ends exactly 30 days before the current date and extends 90 days prior to that.   API Interaction: Makes HTTP requests to the Socrata API, including proper headers for your app token and $where clauses for date filtering.   Pagination: Handles retrieving large datasets by iterating through results using $offset parameters until all available data within the specified date range is fetched.   Data Processing: Converts the JSON response into a pandas DataFrame and formats the inspection_date column.   Saving Data: Exports the cleaned data to CSV and Parquet formats for easy analysis.   Exploring the Data  Once you have the data in a pandas DataFrame (and saved to CSV/Parquet), you can explore it using various tools. You might:     Filter by grade: Find restaurants with A, B, or C grades.   Analyze violation_description: See the most common violations.   Group by cuisine_description: Compare inspection scores across different cuisines.   Map locations: Use the building, street,zipcode, and boro information to visualize restaurant locations and their inspection outcomes.   "
  },
  
  {
    "title": "DuckDB - Diners, Drives, and Databases Part II",
    "url": "/data%20engineering/python/2025/06/08/DuckDB-Diners-Drives-and-Databases-PartII.html",
    "categories": "Data Engineering, Python",
    "tags": "DuckDB, Pandas, data analysis, SQL, Python, testing",
    "date": "2025-06-08 00:00:00 -0400",
    "content": "  Hoping for an A.  In Part I , we got our feet wet by performing initial data explorations on the NYC restaurant inspection dataset directly from a CSV file. We saw how DuckDB allows for quick insights without full data loading into memory.  I felt like I didn’t get to cover everything I wanted to touch in the last post so here I will focus on some of the other capabilities namely as a lightweight, in-process ETL (Extract, Transform, Load) tool.  We’ll cover:     Performing CSV-to-CSV transformations purely with SQL, without ever needing to manually inspect the file.   Converting your transformed data into space-efficient formats like compressed CSV (GZIP) and Parquet, and quantifying the storage savings.   Inspecting the data types within DuckDB and confirming their compatibility before a potential load into a production database like PostgreSQL.   Let’s pick up where we left off, assuming you have your DOHMH.csv file ready and your DuckDB environment set up.  1. The “Blind” CSV-to-CSV Transformation  Imagine you’ve received a large CSV file, and you know it needs some basic cleaning or column selection before you can use it. Just to save a subset. DuckDB is perfect for this “blind” transformation.  We’ll take our DOHMH.csv and perform a few common transformations:     Select a subset of relevant columns.   Rename a column (DBA to Restaurant_Name for clarity).   Filter out any records where the BORO is '0' (an anomalous entry we noticed in Part I).   Write the transformed data to a new CSV file.   import duckdb import os import pandas as pd  # For displaying schemas later  # Define the path to your original CSV file csv_file_path = 'DOHMH.csv' # Define the path for the new, transformed CSV transformed_csv_path = 'DOHMH_transformed.csv'  # Establish a connection to an in-memory DuckDB database con = duckdb.connect()  print(f\"--- Performing CSV-to-CSV Transformation ---\") print(f\"Reading '{csv_file_path}' and writing to '{transformed_csv_path}'...\")  try:     # Use DuckDB's COPY statement with a subquery to transform data     # The subquery selects, renames, and filters data without loading the whole file     transform_query = f'''     COPY (         SELECT             \"CAMIS\",             \"DBA\" AS \"Restaurant_Name\", -- Renaming DBA column             \"BORO\",             \"BUILDING\",             \"STREET\",             \"ZIPCODE\",             \"CUISINE DESCRIPTION\",             \"INSPECTION DATE\",             \"ACTION\",             \"VIOLATION CODE\",             \"VIOLATION DESCRIPTION\",             \"CRITICAL FLAG\",             \"SCORE\",             \"GRADE\"         FROM             '{csv_file_path}'         WHERE             \"BORO\" != '0' -- Filter out anomalous '0' borough             AND \"CAMIS\" IS NOT NULL -- Ensure unique identifier is present     ) TO '{transformed_csv_path}' (HEADER, DELIMITER ',');     '''     con.execute(transform_query)     print(f\"Transformation complete. Transformed data saved to: {transformed_csv_path}\")  except Exception as e:     print(f\"Error during transformation: {e}\")  finally:     con.close()  2. Space-Saving Formats: Compressed CSV &amp; Parquet Once your data is transformed, you often want to store it efficiently. DuckDB makes it trivial to convert your data into compressed formats, which can significantly reduce storage space and often improve read performance for subsequent analytical queries. We’ll compare:  The original DOHMH.csv. Our new DOHMH_transformed.csv. A GZIP-compressed version of the transformed CSV. A Parquet version of the transformed data.   import duckdb import os  # Paths from previous step original_csv_path = 'DOHMH.csv' transformed_csv_path = 'DOHMH_transformed.csv' compressed_csv_path = 'DOHMH_transformed_compressed.csv.gz' # .gz suffix is common for GZIP parquet_path = 'DOHMH_transformed.parquet'  # Ensure the transformed_csv_path exists from the previous step, or run the transformation again # Re-establishing connection for this snippet con = duckdb.connect()  print(f\"\\n--- Comparing File Sizes ---\")  try:     # Get original CSV size     original_size_bytes = os.path.getsize(original_csv_path)     print(f\"Original CSV Size: {original_size_bytes / (1024 * 1024):.2f} MB\")      # Get transformed CSV size     transformed_size_bytes = os.path.getsize(transformed_csv_path)     print(f\"Transformed CSV Size: {transformed_size_bytes / (1024 * 1024):.2f} MB\")     print(f\"  Savings from Transformation (selected columns, filtered rows): \"           f\"{((original_size_bytes - transformed_size_bytes) / original_size_bytes) * 100:.2f}%\")      # Write to GZIP compressed CSV     print(f\"Writing transformed data to GZIP compressed CSV: {compressed_csv_path}\")     copy_to_compressed_csv_query = f\"\"\"     COPY (SELECT * FROM '{transformed_csv_path}') TO '{compressed_csv_path}' (HEADER, DELIMITER ',', COMPRESSION GZIP);     \"\"\"     con.execute(copy_to_compressed_csv_query)     compressed_size_bytes = os.path.getsize(compressed_csv_path)     print(f\"Compressed CSV (GZIP) Size: {compressed_size_bytes / (1024 * 1024):.2f} MB\")     print(f\"  Savings vs. Transformed CSV: \"           f\"{((transformed_size_bytes - compressed_size_bytes) / transformed_size_bytes) * 100:.2f}%\")       # Write to Parquet     print(f\"Writing transformed data to Parquet: {parquet_path}\")     copy_to_parquet_query = f\"\"\"     COPY (SELECT * FROM '{transformed_csv_path}') TO '{parquet_path}' (FORMAT PARQUET);     \"\"\"     con.execute(copy_to_parquet_query)     parquet_size_bytes = os.path.getsize(parquet_path)     print(f\"Parquet Size: {parquet_size_bytes / (1024 * 1024):.2f} MB\")     print(f\"  Savings vs. Transformed CSV: \"           f\"{((transformed_size_bytes - parquet_size_bytes) / transformed_size_bytes) * 100:.2f}%\")       print(f\"\\nTotal savings from original CSV to Parquet: \"           f\"{((original_size_bytes - parquet_size_bytes) / original_size_bytes) * 100:.2f}%\")  except FileNotFoundError as e:     print(f\"Error: A required file was not found. Please ensure '{original_csv_path}' and '{transformed_csv_path}' exist. {e}\") except Exception as e:     print(f\"Error during file compression/conversion: {e}\")  finally:     con.close()  You should see some pretty amazing space savings.  Here are my results more than a 30% savings from the original CSV to the transformed CSV, and then over 90% savings when compressing to GZIP and converting to Parquet. More than 15X reduction, saving in the right format means you can keep a year worth of data in the space you would have used for a single month in the original CSV format.  --- Comparing File Sizes --- Original CSV Size: 124.82 MB Transformed CSV Size: 85.26 MB   Savings from Transformation (selected columns, filtered rows): 31.69% Writing transformed data to GZIP compressed CSV: DOHMH_transformed_compressed.csv.gz Compressed CSV (GZIP) Size: 7.56 MB   Savings vs. Transformed CSV: 91.13% Writing transformed data to Parquet: DOHMH_transformed.parquet Parquet Size: 7.41 MB   Savings vs. Transformed CSV: 91.31%  Total savings from original CSV to Parquet: 94.07%  3. Data Typing &amp; PostgreSQL Compatibility  Before loading data into a production database like PostgreSQL, it’s crucial to confirm that your data types are correct and compatible. DuckDB does a great job of inferring types, but explicit confirmation and potential casting are good practice to avoid surprises during loading.  import duckdb import pandas as pd # For displaying schema as a DataFrame  # Path to our transformed Parquet file (or CSV, it doesn't matter for schema inspection) transformed_data_path = 'DOHMH_transformed.parquet'  # Re-establishing connection con = duckdb.connect()  print(f\"\\n--- Inspecting Schema for PostgreSQL Compatibility ---\")  try:     # Use PRAGMA table_info to get the schema of data from a file     # We implicitly create a temporary table/view from the file for inspection     schema_query = f\"\"\"     PRAGMA table_info('{transformed_data_path}');     \"\"\"     schema_df = con.sql(schema_query).df()      print(\"DuckDB Inferred Schema (from transformed data):\")     print(schema_df.to_string(index=False))      print(\"\\n--- PostgreSQL Type Considerations ---\")     print(\"Here's a general mapping and considerations for PostgreSQL:\")      pg_type_map = {         'BIGINT': 'BIGINT',         'INTEGER': 'INTEGER',         'DOUBLE': 'DOUBLE PRECISION', # or REAL for float4         'VARCHAR': 'VARCHAR(N)',       # N needs to be determined based on max length         'BOOLEAN': 'BOOLEAN',         'DATE': 'DATE',         'TIMESTAMP': 'TIMESTAMP',         'BLOB': 'BYTEA',         'DECIMAL': 'NUMERIC(P, S)'     # P=precision, S=scale needs to be determined     }      for index, row in schema_df.iterrows():         column_name = row['name']         duckdb_type = row['type']         nullable = \"NULL\" if row['null'] == 1 else \"NOT NULL\"          pg_equivalent = pg_type_map.get(duckdb_type.upper(), f\"UNKNOWN_TYPE ({duckdb_type})\")          # Special handling for VARCHAR to suggest length         if duckdb_type.upper() == 'VARCHAR':             # To get actual max length, you'd need to query the data:             # max_len_query = f\"SELECT MAX(LENGTH(\\\"{column_name}\\\")) FROM '{transformed_data_path}';\"             # max_len = con.sql(max_len_query).fetchone()[0]             # pg_equivalent = f\"VARCHAR({max_len or 255})\" # Use a default if max_len is 0 or None             pg_equivalent = \"VARCHAR(255)\" # Common default for text, adjust as needed or calculate max_len          print(f\"- Column: '{column_name}'\")         print(f\"  DuckDB Type: {duckdb_type}\")         print(f\"  PostgreSQL Equivalent: {pg_equivalent}\")         print(f\"  Nullable: {nullable}\")         print(f\"  Considerations: {'Check string max length' if duckdb_type.upper() == 'VARCHAR' else 'Confirm precision/scale' if duckdb_type.upper() == 'DECIMAL' else 'Standard mapping'}\")         print(\"-\" * 30)      print(\"\\nTo load into PostgreSQL, you would typically use a PostgreSQL client library (like Psycopg2 in Python) or a tool like `pg_loader` after connecting to your PostgreSQL database. DuckDB acts as an intermediary here, handling the transformation and type validation.\")  except FileNotFoundError:     print(f\"Error: Transformed data file '{transformed_data_path}' not found. Please ensure the previous steps ran successfully.\") except Exception as e:     print(f\"Error inspecting schema: {e}\")  finally:     con.close()   Explanation of Schema Inspection:  Let’s break down what’s happening in the schema inspection step:          PRAGMA table_info('file_path'):   This handy DuckDB command lets you peek at the schema DuckDB infers from your file—no need to create a table first. It shows you each column’s name, data type, whether it allows NULLs, and more.           Mapping to PostgreSQL:   We walk through a general mapping of DuckDB types to PostgreSQL equivalents. Here are a few things to keep in mind:            VARCHAR length: DuckDB’s VARCHAR is flexible, but PostgreSQL usually wants a specific length (like VARCHAR(255)). To pick the right number, you can run a quick SELECT MAX(LENGTH(column_name)) on your data.       DECIMAL / NUMERIC: If you have columns with decimal numbers (like scores), you’ll want to decide on the total number of digits (PRECISION) and how many come after the decimal point (SCALE).       Date/Time types: DuckDB is pretty good at figuring out dates and timestamps, but double-check if you need a plain DATE or a full TIMESTAMP (and whether you need time zones) for PostgreSQL.             Wrapping Up Part II  In this post, we’ve seen how DuckDB can help you:     Transform data directly from CSVs with SQL, no manual file wrangling required.   Store your results in space-saving formats like compressed CSV and Parquet.   Inspect and prepare your data’s schema for a smooth handoff to databases like PostgreSQL.   I hope this gives you some good ideas for how t use DuckDB as an ETL tool in your data workflows.  I realize this I went in a few different directions here, I like the idea of exploring what is possible and then refining my approach for my use case.  If you have something you are using DuckDB for that I didn’t cover please share it.  I love to hearing what is working for others.  "
  },
  
  {
    "title": "DuckDB - Diners, Drives, and Databases Part I",
    "url": "/data%20engineering/python/2025/06/06/DuckDB-Diners-Drives-and-Databases.html",
    "categories": "Data Engineering, Python",
    "tags": "DuckDB, Pandas, data analysis, SQL, Python, testing",
    "date": "2025-06-06 00:00:00 -0400",
    "content": "  Hoping for an A.  Pandas has been my go-to tool for data manipulation in Python for years. So great for me, right?  End of blog post.  Except…  “If the only tool you have is a hammer, you will see every problem as a nail.” This dollop of wisdom, often attributed to Maslow, reminds us that the solutions we find are often shaped by the tools we have available. I’m not calling Pandas a hammer; it’s an incredible combination of tools that has truly changed how millions of people, myself included, get work done. My point is simply this: opening yourself up to different solutions can lead you to achieve different—and hopefully better—results.  That’s precisely where DuckDB shines. I’ve been using it in my own data pipelines to speed up transformations and prep reports for joining with other databases. And honestly? I know I’m only scratching the surface of its possibilities. So I thought I would try a few simple jobs with DuckDB, and maybe this exploration will prove useful to others too.  If you’ve been coding in Python for a while, you’re likely familiar with traditional database solutions like PostgreSQL or SQLite. DuckDB stands out from the crowd because it offers virtually no setup, delivers blazing-fast performance, and can directly pull entire datasets from a multitude of formats.  Let’s put it to the test with some real-world data anyone can access. We’ll be using the NYC Department of Health’s restaurant inspection results:  https://data.cityofofnewyork.us/Health/DOHMH-New-York-City-Restaurant-Inspection-Results/43nn-pn8j/about_data  To get the data, just click the ‘Actions’ button in the top right corner on that page, select ‘Query Data’, and then hit ‘Export’ to download the CSV file. It should appear in your downloads folder in less than a minute.    !!Warning!! If you have a favorite restaurant, you might actually not want to look it up in this dataset. Ignorance, bliss… !!Warning!!    Optional but Recommended: Simplify Your File Path  If you’re already comfortable navigating file paths and extensions, feel free to skip this step. Otherwise, for easier referencing in our code, I recommend renaming the downloaded file. It will likely have a long name like DOHMH_New_York_City_Restaurant_Inspection_Results_202506XX.csv. Let’s simplify it to just DOHMH.csv.    Getting Your Environment Set Up:  First, install the necessary libraries using uv (if you have it) or pip.  uv pip install duckdb pandas # If you don't have uv, just use: pip install duckdb pandas   Initial Data Exploration: First Counts Now that we have our data and tools ready, let’s dive into some basic, yet incredibly insightful, questions about our NYC restaurant inspection dataset. Instead of loading the entire massive file into memory with pandas (which can be slow for very large datasets and consume a lot of RAM), we’ll leverage DuckDB’s ability to directly query the CSV file. This keeps our memory footprint low and our queries fast.  Let’s start by getting a quick idea of our dataset size:  import duckdb import os  # Define the path to your CSV file. # Make sure 'DOHMH.csv' is in the same directory as your Python script, # or provide the full absolute path. csv_file_path = 'DOHMH.csv'   # Establish a connection to an in-memory DuckDB database. # Close it after use in this block. con = duckdb.connect()  # Total Number of Inspections using DuckDB print(\"--- Dataset Overview ---\") total_inspections_query = f\"\"\" SELECT COUNT(*) AS total_inspections FROM '{csv_file_path}'; \"\"\" total_inspections = con.sql(total_inspections_query).fetchall()[0][0] print(f\"Total number of inspection records (DuckDB): {total_inspections:,}\")  con.close() # Close the DuckDB connection for this snippet   We can use Pandas to confirm our results for the total rows:  import pandas as pd import os  # Make sure this matches your file path csv_file_path = 'DOHMH.csv'  df = pd.read_csv(csv_file_path) print(df.shape)   Your total rows should match the total from DuckDB. In my case it was 285,210, but the actual number may depend on when you downloaded the file. Let’s put this in perspective: you would have to inspect a restaurant every 1.5 minutes, 24/7, for an entire year to reach that number of inspections!  TLDR: What Makes this Special? I want to point out something important about how DuckDB works here: it allows you to run SQL queries directly on CSV files without needing to load the entire dataset into memory. This offers a significant, often ephemeral, performance and memory advantage. In other words, you get the benefit of lower memory use during the query. However, this power also comes with an important practice: remembering to close your connection to the database.  In the examples above, we’ve diligently closed our connection using con.close() after running our queries. We’ll strive to follow these best practices, but if a small mistake slips through and I leave a connection open, well, that’s just too bad, isn’t it? Fortunately (or perhaps unfortunately for accountability), there are no database inspectors handing out fines for that.  Anyway we lost that advantage when we checked the table size with Pandas.  Let’s review a few more basic queries.  Restaurants by Borough We can easily count the number of inspections by borough, but some places have been inspected on multiple occasions. Let’s compare total inspections by borough to total restaurants by borough using the unique CAMIS number to find how many restaurants in each borough have been inspected. import duckdb import pandas as pd  # Define the path to your CSV file. csv_file_path = 'DOHMH.csv'  # Connect to DuckDB (in-memory) con = duckdb.connect()  # Inspections by Borough borough_inspections = con.execute(\"\"\"   SELECT     \"BORO\",     COUNT(*) AS total_inspections_in_borough   FROM read_csv_auto(?)   GROUP BY \"BORO\"   ORDER BY total_inspections_in_borough DESC \"\"\", [csv_file_path]).df() print(borough_inspections.to_markdown(index=False))  # Unique Restaurants by Borough (using CAMIS) unique_restaurants = con.execute(\"\"\"   SELECT     \"BORO\",     COUNT(DISTINCT \"CAMIS\") AS unique_restaurants_count   FROM read_csv_auto(?)   GROUP BY \"BORO\"   ORDER BY unique_restaurants_count DESC \"\"\", [csv_file_path]).df() print(unique_restaurants.to_markdown(index=False))  con.close()                 BORO       total_inspections_in_borough                       Manhattan       105410                 Brooklyn       74822                 Queens       68940                 Bronx       26017                 Staten Island       10006                 0       15                         BORO       unique_restaurants_count                       Manhattan       11980                 Brooklyn       7828                 Queens       6945                 Bronx       2556                 Staten Island       1126                 0       15           Well that is Unusual Isn’t it? What stands out for me here is all the boroughs appear to have about 10% as many unique restaurants as they do inspections, except the Bronx.  Let’s review that.  import duckdb import pandas as pd  # Define the path to your CSV file. csv_file_path = 'DOHMH.csv'  # Connect to DuckDB (in-memory) con = duckdb.connect()  # Get total inspections by borough borough_inspections = con.execute(\"\"\"   SELECT     \"BORO\",     COUNT(*) AS total_inspections_in_borough   FROM read_csv_auto(?)   GROUP BY \"BORO\"   ORDER BY total_inspections_in_borough DESC \"\"\", [csv_file_path]).df()  # Get unique restaurants by borough unique_restaurants = con.execute(\"\"\"   SELECT     \"BORO\",     COUNT(DISTINCT \"CAMIS\") AS unique_restaurants_count   FROM read_csv_auto(?)   GROUP BY \"BORO\"   ORDER BY unique_restaurants_count DESC \"\"\", [csv_file_path]).df()  con.close()  # Merge and calculate percentage merged = pd.merge(     borough_inspections,     unique_restaurants,     on='BORO',     how='outer' ).fillna(0)  merged['Percentage_Restaurants_per_Inspection'] = (     merged['unique_restaurants_count'] / merged['total_inspections_in_borough'] * 100 ).round(2)  # Format percentage as string with % merged['Percentage_Restaurants_per_Inspection'] = merged['Percentage_Restaurants_per_Inspection'].map('{:.2f}%'.format)  print(\"Percentage of Unique Restaurants per Inspection by Borough\") print(merged[['BORO', 'total_inspections_in_borough', 'unique_restaurants_count', 'Percentage_Restaurants_per_Inspection']].to_markdown(index=False))   Percentage of Unique Restaurants per Inspection by Borough                BORO       total_inspections_in_borough       unique_restaurants_count       Percentage_Restaurants_per_Inspection                       0       15       15       100.00%                 Manhattan       105410       11980       11.37%                 Staten Island       10006       1126       11.25%                 Brooklyn       74822       7828       10.46%                 Queens       68940       6945       10.07%                 Bronx       26017       2556       9.82%           Insights:  This table clearly quantifies the observation! Excluding the “0” borough (which shows a 100% ratio, likely indicating a data anomaly where unique restaurants equal inspection count), we can analyze the main boroughs:     The percentages for Manhattan (11.37%), Staten Island (11.25%), Brooklyn (10.46%), and Queens (10.07%) are all quite close, generally hovering around 10-11% unique restaurants per total inspections.   The Bronx, at 9.82%, is indeed the lowest among the five main boroughs.   This is the part of data analysis where I like to speculate wildly about why patterns like that emerge, but today I am going to show restraint.   Let’s Talk Donuts  How many Donut Shops are in New York and how many of them are Dunkin?  import duckdb import pandas as pd  # Connect to DuckDB and set CSV file path con = duckdb.connect() csv_file_path = 'DOHMH.csv' min_year = 2024  # Donut Shop Dominance: Dunkin' vs. Others (Since 2024) donut_query = f\"\"\" SELECT   CASE     WHEN \"DBA\" ILIKE '%DUNKIN%' THEN 'Dunkin'' (Locations)'     ELSE 'Other Donut Shops (Locations)'   END AS donut_category,   COUNT(DISTINCT \"CAMIS\") AS unique_donut_shop_locations FROM read_csv_auto('{csv_file_path}') WHERE   (\"CUISINE DESCRIPTION\" ILIKE '%DONUT%' OR \"CUISINE DESCRIPTION\" ILIKE '%DOUGHNUT%')   AND CAST(strftime('%Y', \"INSPECTION DATE\") AS INTEGER) &gt;= {min_year} GROUP BY donut_category ORDER BY unique_donut_shop_locations DESC; \"\"\" donut_df = con.execute(donut_query).df() print(donut_df.to_markdown(index=False))  # Names of Other Donut Shops (Since 2024, Excluding Dunkin') other_donut_shops_query = f\"\"\" SELECT DISTINCT \"DBA\", \"CAMIS\" FROM read_csv_auto('{csv_file_path}') WHERE   (\"CUISINE DESCRIPTION\" ILIKE '%DONUT%' OR \"CUISINE DESCRIPTION\" ILIKE '%DOUGHNUT%')   AND \"DBA\" NOT ILIKE '%DUNKIN%'   AND CAST(strftime('%Y', \"INSPECTION DATE\") AS INTEGER) &gt;= {min_year} LIMIT 20; \"\"\" other_donut_shops_df = con.execute(other_donut_shops_query).df() print(other_donut_shops_df.to_markdown(index=False))  con.close()   Donut Shops (Since 2024): Dunkin’ vs. Others in NYC                donut_category       unique_donut_shop_locations                       Dunkin' (Locations)       500                 Other Donut Shops (Locations)       33           The 800 lb Jelly Donut in the Room  Out of 533 Donut Shops in the city more than 93% are Dunkin, and of those 33 non Dunkin shops 8 of them have the word “Krispy” in their name.  Anyway if you made it this far, I hope this was a help to you. I would love to hear your thoughts about Duckdb or Donuts or anything else. "
  },
  
  {
    "title": "Next Train to Alberquerque: Track NYC Subway Arrivals in Real-Time with Python",
    "url": "/data%20engineering/python/2025/06/01/Next-Train-To-Alberquerque.html",
    "categories": "Data Engineering, Python",
    "tags": "MTA, subway, GTFS-Realtime, Python, data",
    "date": "2025-06-01 12:00:00 -0400",
    "content": "      If only there was a better way.   Tired of anxiously peering down a dark subway tunnel, wondering when your train will actually arrive, or opening an app that asks you to scroll through every line to find the one you need?  With a bit of Python and access to the MTA’s live data, you can build your own simple subway arrival tracker.  I have heard of some nice projects that people have used to keep a little sign in their apartment that tells them when the next train is coming.  If you are instested in that, this will help get you started.  The MTA used to require an API key for accessing their real-time data, but now you can access it freely. Which is exactly one less thnig you need to worry about.  This post will guide you through:     Fetching live data from the MTA.   Understanding the basics of GTFS-Realtime (the data format).   Writing a Python script to find specific train arrivals.   Most importantly, how to customize this script for YOUR station, YOUR line, and YOUR commute!   I would recommend getting the code to run first and then customizing it to your local stations.  Once you have a working script you can see how the see how your changes affect the output.  What You’ll Need     Python: If you don’t have it, download it from python.org.   Basic Python Knowledge: You should be comfortable with variables, functions, loops, and conditional statements.   Two Python Libraries:            requests: For fetching data from the web.       gtfs-realtime-bindings: For making sense of the MTA’s real-time data format.           You can install these by opening your terminal or command prompt and typing:  pip install requests pip install gtfs-realtime-bindings   Behind the Scenes: MTA &amp; GTFS-Realtime  The Metropolitan Transportation Authority (MTA) generously provides real-time data feeds for its services. This data uses a standard called GTFS-Realtime. Think of GTFS (General Transit Feed Specification) as a universal language for public transportation schedules, and GTFS-Realtime as the live, up-to-the-minute updates to those schedules (like delays, changed arrival times, and vehicle positions).  This data is typically provided in a compact format called Protocol Buffers, which is where our gtfs-realtime-bindings library comes in handy to parse it.  The MTA has different feeds for different sets of subway lines. For our main example, we’ll use the feed for the N, Q, R, W lines, but I’ll show you how to find others too.  The python script  Let’s look at a Python script that checks for a northbound Q train arriving at Newkirk Plaza. Then, we’ll break down how to modify it.  import requests from google.transit import gtfs_realtime_pb2 import time # To interpret timestamps   ## --- CONFIGURATION: YOU'LL CHANGE THIS! ---  ## Find other feed URLs here:  https://api.mta.info/#/System%20Data/TripUpdates/get_tripUpdatesByRoute ## Example: NQRW lines TRANSIT_FEED_URL = \"https://api-endpoint.mta.info/Dataservice/mtagtfsfeeds/nyct%2Fgtfs-nqrw\"   ## For direction: \"N\" for Northbound, \"S\" for Southbound (used in trip_id convention) DIRECTION_ABBREVIATION = \"N\"  ## CRITICAL: Find your stop_id from MTA's static GTFS data (stops.txt)  ## Example: Newkirk Plaza Northbound on Brighton Line (Q) TARGET_STOP_ID = \"D22N\" TARGET_STATION_NAME = \"Newkirk Plaza\" # For print messages  ## How many minutes in advance do you want to be notified? (e.g., 30 minutes) ARRIVAL_WINDOW_MINUTES = 30 ## --- END CONFIGURATION ---  def get_live_mta_data(url):      try:         response = requests.get(url)         response.raise_for_status()         return response.content     except requests.exceptions.RequestException as e:         print(f\"Error fetching MTA data: {e}\")         return None  def parse_gtfs_realtime_feed(feed_content):      feed = gtfs_realtime_pb2.FeedMessage()     feed.ParseFromString(feed_content)     return feed  def check_train_arrivals():     #Checks for specified train arrivals at the target stop.     raw_feed = get_live_mta_data(TRANSIT_FEED_URL)     if not raw_feed:         print(\"Could not fetch MTA feed.\")         return      feed = parse_gtfs_realtime_feed(raw_feed)     now = time.time()     found_arrival = False      print(f\"--- Checking arrivals for Route {ROUTE_ID_TO_CHECK} ({DIRECTION_ABBREVIATION}B) at {TARGET_STATION_NAME} ({TARGET_STOP_ID}) ---\")     print(f\"Current time: {time.strftime('%I:%M:%S %p')}\")       for entity in feed.entity:         if entity.HasField('trip_update'):             trip_update = entity.trip_update             trip = trip_update.trip              # 1. Check for the correct Route ID             if trip.route_id == ROUTE_ID_TO_CHECK:                 # 2. Check for Direction (using trip_id convention)                 # MTA trip_ids often have '..N' or '..S' in the last segment                 # e.g., \"A20230515WKD_000800_Q..N03R\"                 is_correct_direction = False                 try:                     # Trip ID structure can vary. This is a common MTA pattern.                     # Example: some_prefix_ROUTEID..DIRECTIONcode_suffix                     if \"..\" in trip.trip_id:                         parts = trip.trip_id.split(\"..\")                         if len(parts) &gt; 1 and parts[-1].startswith(DIRECTION_ABBREVIATION):                             is_correct_direction = True                     # Fallback for simpler trip_ids like those on LIRR/MetroNorth if adapting later                     # elif trip.trip_id.endswith(DIRECTION_ABBREVIATION):                     # is_correct_direction = True                 except Exception as e:                     # print(f\"Could not parse trip_id for direction: {trip.trip_id} - {e}\")                     pass # Continue, maybe rely on stop_id direction if specific enough                  # If direction is confirmed or stop_id itself is direction-specific                 if is_correct_direction or TARGET_STOP_ID.endswith(DIRECTION_ABBREVIATION):                     for stop_time_update in trip_update.stop_time_update:                         # 3. Check for the Target Stop ID                         if stop_time_update.stop_id == TARGET_STOP_ID:                             arrival = stop_time_update.arrival                             if arrival and arrival.time &gt; 0: # Ensure arrival time exists                                 arrival_time_unix = arrival.time                                 time_to_arrival_seconds = arrival_time_unix - now                                 arrival_time_readable = time.strftime('%I:%M:%S %p', time.localtime(arrival_time_unix))                                  if 0 &lt;= time_to_arrival_seconds &lt;= (ARRIVAL_WINDOW_MINUTES * 60):                                     minutes_to_arrival = round(time_to_arrival_seconds / 60)                                     print(f\"  ALERT! {ROUTE_ID_TO_CHECK} train ({DIRECTION_ABBREVIATION}B) for {TARGET_STATION_NAME}\")                                     print(f\"    Trip ID: {trip.trip_id}\")                                     print(f\"    Estimated Arrival: {arrival_time_readable} (in approx. {minutes_to_arrival} min)\")                                     found_arrival = True                                 elif time_to_arrival_seconds &lt; 0 and time_to_arrival_seconds &gt; -120: # Just arrived (within last 2 mins)                                     print(f\"  INFO: {ROUTE_ID_TO_CHECK} train ({DIRECTION_ABBREVIATION}B) likely JUST ARRIVED/AT {TARGET_STATION_NAME}.\")                                     print(f\"    Scheduled arrival was {arrival_time_readable}.\")                                     found_arrival = True                             break # Found our stop for this trip, move to next trip entity          if not found_arrival:         print(f\"No {ROUTE_ID_TO_CHECK} trains ({DIRECTION_ABBREVIATION}B) currently reporting an upcoming arrival at {TARGET_STATION_NAME} within {ARRIVAL_WINDOW_MINUTES} minutes.\")     print(\"---------------------------------------------------\\n\")  if __name__ == \"__main__\":     while True:         check_train_arrivals()         # How often to check (in seconds)         # MTA feeds update frequently, but every 30-60 seconds is reasonable         time.sleep(30)  "
  },
  
  {
    "title": "Lemon Labor Lost: A Quest for the Perfect Cookie",
    "url": "/food/personal/2025/05/29/Lemon-labor-lost.html",
    "categories": "Food, Personal",
    "tags": "lemon cookies, food review, maximizer, satisficer",
    "date": "2025-05-29 12:00:00 -0400",
    "content": "      Twas a cookie, with scant but the scent of a lemon.   For weeks now, I’ve been on a quiet, personal quest: the search for the perfect lemon cookie. It’s not some grand, dramatic saga; it’s just a simple desire to find that one, cookie that delivers a bright, zesty, and utterly satisfying lemon experience. No, I don’t know what is wrong with me. I just want what I want.  So for a while now, I have been going to different grocery stores and looking for lemon cookie mixes or lemon-flavored cookies. On one trip, the only ones with icing I could find were lemon Oreos. I thought, well if this is the only one’s they have, I will give them a try. I like regular Oreos I will probably like these too.  I opened the package on the walk home. The cookie part had no lemon flavor, it was like one of those regular ‘Golden’ Oreos. The filling was okay. It had a mild lemon flavor but not what I was hoping for. Is it Oreo’s that are disappointing or my high expectations? Obviously this is a huge company and making a mass market appeal cookie is going to be the priority, not trying to ease the longing of the few people in the world who are trying to rediscover and idealized moment. I understand in psychology, some people are maximizers and some are satisficers. This is one of the times I am a maximizer. I don’t want to be, because maximizers are always looking for the best. But the rub is that even when they find it, they have a comparatively less enjoyable experience than satisficers. So I am fated to fail?  I don’t know what specific lemon cookie from my past set this high, yet seemingly unachievable, bar. But its memory drives my continued search. It might be that I am nostalgic or have unrealistic expections. Anyway if anyone knows of lemon cookie they really enjoy I’d love to hear about it. There are some lemon cookies I like. These are great but they don’t have any icing.  Update:  I am happy to report that I have found a lemon treat that satisfies my craving! While technically not a cookie, the Bonne Maman Lemon Tartlets have a delightful lemon flavor in the shortbread crust that is exactly what I was looking for. The lemon filling is great, not the icing I was thinking I was looking for but very nice.  Almost like a little creme brulee. Weirdly the link is mislabeled as “Raspberry Tartlets” but the image and product description confirm they are indeed lemon tartlets.        The end of my quest? Bonne Maman Raspberry Tartlets   So, for now, my quest has come to a satisfying end. Maybe I am not fated to be a perpetually unsatisified maximizer, which is real relief. However, I am always open to trying other lemon cookies, so please keep the recommendations coming!  "
  },
  
  {
    "title": "Automated SQL Backups to Google Drive with Python",
    "url": "/development/automation/databases/2025/05/26/Automated-SQL-Backups-to-Google-Drive.html",
    "categories": "Development, Automation, Databases",
    "tags": "python, google-drive, backup, sql, automation",
    "date": "2025-05-26 12:00:00 -0400",
    "content": "Protecting your data is crucial, and having automated backups is a cornerstone of any robust data strategy. In this post, I’ll walk you through a Python script that automatically backs up your SQL database (in .gz format) to Google Drive. This ensures your backups are stored securely offsite and are easily accessible when needed.  As always there are a lot of cateats to consider, like over using your Google Drive storage quota, in this case my total backups are less than 1GB.  A service like rclone can you help you manage your backups with Google Drive and other cloud stoarge providers. without having to write your own code. This is a longer process and it involves running a cron job to fully automate.  I usually advocate for the simple and straightforward approach, but in this case, I think there is value in scripting the process.  Both for learning and flexibility. There are a lot of great tools for backing up your files, they can also consume a lot of resources.  Why Automate Backups to Google Drive?     Offsite Storage: Google Drive provides a secure and reliable offsite location for your backups, protecting against local hardware failures or disasters.   Automation: Automating the backup process ensures that backups are performed regularly without manual intervention.   Version History: Google Drive keeps a history of file versions, allowing you to restore to a specific point in time if necessary.   Accessibility: Your backups are accessible from anywhere with an internet connection.   Prerequisites  Before you begin, make sure you have the following:     Python 3.7+: Ensure you have Python installed.   Google Cloud Project: You’ll need a Google Cloud project with the Google Drive API enabled.   Service Account: Create a service account in your Google Cloud project and download the JSON key file. This key file will be used to authenticate your script with Google Drive.        Required Python Libraries: Install the necessary libraries using pip:      pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib           The Python Script: Automating the Backup  Here’s the Python script that handles the automated backup process. I’ve added comments inline to explain each step.  You need to combine the steps into a single script, and you can run it manually or set it up to run on a schedule using cron or another task scheduler.  import os import glob import datetime from google.oauth2.service_account import Credentials from googleapiclient.discovery import build from googleapiclient.http import MediaFileUpload from googleapiclient.errors import HttpError  — CONFIGURATION — LOCAL_BACKUP_DIRECTORY = \"/home/files/folders/nestedfolder/db_backups\"  # Directory containing .gz backups FILE_PATTERN = \"*.gz\"  # Pattern to match .gz files GOOGLE_DRIVE_FOLDER_ID = \"YOUR_GOOGLE_DRIVE_FOLDER_ID\"  # Replace with your Google Drive folder ID SERVICE_ACCOUNT_FILE = \"path/to/your/service_account.json\"  # Path to your service account key file   — AUTHENTICATION — SCOPES = ['https://www.googleapis.com/auth/drive.file']  # Scope for Google Drive access credentials = Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)  # Authenticate with service account service = build('drive', 'v3', credentials=credentials)  # Build the Google Drive service   — UPLOAD FUNCTION —  def upload_backup(filename, folder_id):     #Uploads a file to Google Drive.     file_metadata = {'name': filename, 'parents': [folder_id]}  # Metadata for the file     media = MediaFileUpload(filename, mimetype='application/gzip', resumable=True)  # File to upload     try:         file = service.files().create(body=file_metadata, media=media, fields='id').execute()  # Upload the file         print(f\"File ID: {file.get('id')} uploaded successfully.\")  # Print success message     except HttpError as error:         print(f\"An error occurred: {error}\")  # Print error message   — MAIN FUNCTION —   def main():     #Main function to find backup files and upload them.     files = glob.glob(os.path.join(LOCAL_BACKUP_DIRECTORY, FILE_PATTERN))  # Find all .gz files     if not files:         print(\"No backup files found.\")  # If no files found, print message         return      for filepath in files:  # Loop through each file         filename = os.path.basename(filepath)  # Get the filename         print(f\"Uploading {filename}...\")  # Print uploading message         upload_backup(filepath, GOOGLE_DRIVE_FOLDER_ID)  # Upload the file  if __name__ == \"__main__\":     main()  # Run the main function    Automating the Backup Process with Cron  To automate the backup process, you can use a cron job (on Linux/macOS). Cron allows you to schedule tasks to run automatically at specific intervals.  Setting Up a Cron Job          Open the Crontab File:      Open your terminal and type the following command:      crontab -e           This will open the crontab file in a text editor. If this is your first time using crontab, you may be prompted to select an editor.           Understand Crontab Syntax:      Each line in the crontab file represents a scheduled task and follows this format:      minute hour day_of_month month day_of_week command                  minute: (0-59)       hour: (0-23)       day_of_month: (1-31)       month: (1-12)       day_of_week: (0-6, 0 is Sunday)       command: The command to execute           You can use special characters:             *: Represents “every”.       ,: Specifies a list of values.       -: Specifies a range of values.       /: Specifies a step value.                Add the Cron Job Entry:      Add a line to the crontab file to schedule your Python script. For example, to run the script every day at 2:00 AM, add the following line:      0 2 * * * python /path/to/your/script.py &gt; /path/to/backup.log 2&gt;&amp;1                  0 2 * * *: This schedules the task to run at 2:00 AM every day.       python /path/to/your/script.py: This is the command to execute your Python script.  Replace /path/to/your/script.py with the actual path to your script.       &gt; /path/to/backup.log 2&gt;&amp;1: This redirects the output of the script (both standard output and standard error) to a log file named backup.log.  This is helpful for troubleshooting.                Save the Crontab File:      Save the crontab file. The changes will be applied automatically.           Verify the Cron Job:      You can verify that the cron job has been added by running the following command:      crontab -l           This will list all the cron jobs in your crontab file.      Important Notes:     Full Paths: Always use full paths to the python executable and your script in the cron job entry.   Logging: Redirecting the output to a log file is highly recommended for debugging purposes.   Testing: Test your cron job by setting it to run more frequently (e.g., every minute) and checking the log file to ensure that it’s working correctly.   Example:  To run the backup script located at /home/user/backup_script.py every day at 3:30 AM and log the output to /home/user/backup.log, the cron job entry would be:  30 3 * * * python /home/user/backup_script.py &gt; /home/user/backup.log 2&gt;&amp;1   "
  },
  
  {
    "title": "The Legend of CRISPR: Long Ago and Far Away a Young Prokaryote...",
    "url": "/technology/science/2025/05/15/Sometimes-Good-Things-Happen.html",
    "categories": "Technology, Science",
    "tags": "CRISPR, Gene Editing, Biotechnology, Health, python",
    "date": "2025-05-15 12:00:00 -0400",
    "content": "Sort of a fairytale… Almost everything about CRISPR sounds magical, even its origins.  Some of the first living organisms, are attacked by viruses, few survive. Some evolve an ability that allows them to snip the RNA out of the virus and hold on to it protecting themselves from future attacks. If they are attacked by the virus again, they can change the genetics of the virus, ending the threat.  Thousands of years later, humans learn this secret and figure out how to use it save the lives of others.        This week I read that a child that had virtually no chance of survival, seems to have been saved by this technology. They call the kid KJ, and if it worked for him they might be able to save thousands or even millions of other lives. The speficific disease is called AADC deficiency and it is a rare genetic disorder that affects the brain’s ability to produce dopamine and serotonin. But there is good reason to believe the same techniques could be used to treat other diseases as well.  That is not even the only use for CRISPR. Here is a python library used to help identify similar looking plants potentially to help identify invasive species more reliably. I am sure there will be countless other uses for this technology. If you are interested in exploring more with python, here is a link to the DepMap portal. It is a great resource for exploring CRISPR in the context of cancer research. A great primer on Bioinformatics is the Biopython library  "
  },
  
  {
    "title": "Brighter But Not Harmful: The 'uv' Python Installer",
    "url": "/development/python/2025/04/25/Brighter-But-Not-Harmful.html",
    "categories": "Development, Python",
    "tags": "python, uv, dependency-management, pip, package-manager, rust",
    "date": "2025-04-25 12:00:00 -0400",
    "content": "The Effective and Unobtrusive ‘uv’ Python Package Manager  I really like ‘uv’. I know controversial opinion, but I am not afraid of what the bureaucrats in city hall will say. I never really felt like dependencies took too long to download. But when I saw uv in action, I mourned the collective hours of all the python users who had the misfortune of coding before ‘uv’, a new, blazing-fast Python package installer and resolver from Astral, the creators of the Ruff linter. It’s designed to be a drop-in replacement for common pip and pip-tools workflows, but significantly faster.  Why is uv a Big Deal?          Speed: This is uv’s headline feature. It’s written in Rust and leverages parallelism and smart caching to install and resolve dependencies much quicker than traditional tools. For large projects or CI/CD pipelines, this can mean saving substantial amounts of time.           Drop-in Replacement (Mostly): uv aims to be compatible with existing requirements.txt files and pyproject.toml standards. You can often use uv pip install … much like you would pip install ….           Unified Tooling: It can act as both a package installer (like pip) and a resolver for pinning dependencies (like pip-compile from pip-tools). It also includes capabilities for managing virtual environments.           Modern Foundation: Built with performance and correctness in mind from the ground up.      What Can You Do With uv?          uv pip install -r requirements.txt: Install dependencies from a requirements file, but faster.           uv pip compile pyproject.toml -o requirements.txt: Resolve dependencies and generate a locked requirements.txt file.           uv venv: Create virtual environments quickly.      Sometimes you think the world is going in the wrong direction. And then you see something like uv and you think, maybe it is not so bad after all.  Here are the docs if you want to learn more: uv docs.  "
  },
  
  {
    "title": "Judy The Wise",
    "url": "/2024/10/10/Judy-The-Wise.html",
    "categories": "",
    "tags": "",
    "date": "2024-10-10 00:00:00 -0400",
    "content": "No technology in this post—just some thinking and reflecting. If this resonates with you, great; if not, perhaps another post will. This is the story of how a seemingly trivial moment on Judge Judy illuminated the often-unseen dynamics of power that shape our relationships and societies. I didn’t know how little I understood people until I watched this one episode.  Too many years ago to count, I was watching Judge Judy. If you’re not familiar, the appeal of the show and shows like it lies in watching someone with an undeservedly high opinion of themselves be corrected. Usually there’s a small amount of money in question—I think it’s put up by the show—which helps make the judge’s decision seem a little weightier than just receiving a talking-to. The judge is really the star of the show. The audience in these courtrooms is more or less for appearance and to provide somewhere for the judge’s jokes to land. There’s a bailiff, and their job is to agree with the judge.  I never really thought much of these shows, honestly. I know I’ve obviously given them considerable thought since then. Maybe it was a right time, right place event; maybe it was just something I already knew but didn’t realize. I should point out that although I’ve only seen this episode once, I’ve tried to find it again but have never succeeded. According to Google, there are more than 7,000 episodes of Judge Judy. I’ve seen more episodes of the show than I’d like to admit, though I haven’t seen one in several years. Cord cutting has more to do with that than taste, I assure you. Before this particular episode, I even used to harbor a slightly snobby attitude towards the show, dismissing it as mere entertainment. But that’s the nice thing about being open to new experiences: our opinions can change and hopefully grow.  The episode I’m talking about involved two women who were roommates. The first one, I’ll call her Generous. Generous explained how she invited her friend (named here as Free-Loader) to live with her. She knew Free-Loader was unemployed and didn’t have anywhere to go. She offered Free-Loader the chance to move in with her and asked her to pay back half the rent once she started working. Free-Loader agreed and moved in, but made no effort to find work, despite constantly promising she would. After about eight months of this, Free-Loader moved out, leaving a debt of about $5,000. Generous seemed genuinely hurt, which was understandable, but it also made her appear somewhat gullible to me at first. It seriously took her eight months to see through this?  The beginning of the case was pretty standard, except that Free-Loader seemed kind of bored by the proceedings. She was either giving one-word answers or mumbling while Generous was speaking. After establishing the timeline and how the amount of money was calculated, Judge Judy spoke directly to Free-Loader. This is where the exchange became truly illuminating. Judge Judy, cutting through Free-Loader’s evasions, directly asked how she intended to pay back Generous. Free-Loader, with a straight face and an almost comical lack of self-respect, replied, “I went to my money tree but it was empty.” Even for a show where you are looking for someone to be the villain, this seemed over the top. If she was speaking plainly, she would have said, “I never planned to pay her back.” Judge Judy seemed unfazed and, seemingly as a courtesy, prompted her, “Could you say that one more time?” And Free-Loader, unflustered, reiterated, “I was going to get it from my money tree but when I got there, there was no money.” At this point, Judge Judy’s patience evaporated. “Okay, I’m done with you,” she declared, “I don’t need anything else. I’m going to talk to the plaintiff now.”  This moment wasn’t just about the absurd excuse; it was the definitive proof of Free-Loader’s complete detachment from any sense of obligation, financial or personal. Her true nature as a self-serving individual was laid bare. But the real wisdom came next, when Judge Judy turned her attention to Generous.  “Let me ask you something,” Judge Judy began, “when you met this person, she had already done what she is doing to you, to other people?” Generous looked confused at first, but a dawning realization flickered across her face as she started to understand. Judge Judy pressed on, “When she did this to other people, you used to think it was funny, didn’t you?” Generous nodded, reluctantly. “But the difference is, you never thought she would do this to you; you thought you were in on the joke. You thought you were on the inside. But when she turned around and did it to you, it wasn’t so funny anymore.” Judge Judy concluded, “So let me tell you what I’m going to do here, I’m going to rule in your favor. But you’re never going to see the money from this person.”  The Cost of Being “On the Inside” This exchange was the epiphany. Initially, the case seemed to me a simple matter of someone taking advantage of a friend, and I had viewed Generous as simply gullible and overly trusting. However, Judge Judy’s pointed questioning of Generous, and her unmasking of Free-Loader’s character, revealed a deeper, more uncomfortable truth that profoundly impacted me. Generous wasn’t just a victim of debt; she was a victim of misplaced loyalty rooted in a selective tolerance of Free-Loader’s behavior. I can only assume Generous felt a real, genuine sense of betrayal and surprise when she realized Free-Loader was never going to pay her back, nor even express gratitude for the profound generosity she had been given. Generous had implicitly sided with Free-Loader’s exploitative actions when they were directed at others, finding amusement in the “joke” because she believed herself immune—part of an exclusive circle, part of the coveted “in-group” that was safe from Free-Loader’s predatory behavior.  It was this realization that made me understand that much of what we call bullying isn’t about direct harm to a specific victim as its primary goal. Instead, much of bullying is performative; demeaning the victim is often merely the sideshow, not the actual objective. The true aim is to establish control by implicitly setting an “in-group” and an “out-group.” This is precisely where the complex dynamic of self-serving individuals, the manipulative bully, truly reveals itself. It’s not just about some people being willing victims; often, they genuinely believe having a bully “fight” for them, or even just tolerating their behavior when directed elsewhere, makes them stronger, or at least exempt from the bully’s wrath. They see shared circumstances or a shared narrative, failing to recognize that for the bully, the alliance is merely a means to an end, and that anyone, even those perceived to be “on their side,” is ultimately expendable. The facade of friendship, of being “on the same team,” crumbled under the weight of Free-Loader’s utter lack of accountability and empathy, and Generous’s realization that she was just another mark.  Looking back, I can see echoes of Generous in my own past. I think almost everyone has experienced some form of bullying, and it can be unpleasant to remember yourself this way: vulnerable, weak, unsure. It’s not just the memory of the external event, but the jarring disconnect from the person we are now, or aspire to be. We struggle to reconcile our present strength and agency with that past self who felt powerless, leaving us to still wonder what made us the target or what we could have said differently. This rejection of our past selves, that sense of having overcome or moved past that vulnerability, often blunts our ability to sympathize with others being victimized. It soothes our egos, assuring us we are not ‘that person’ anymore, while simultaneously instilling the fear that we could still be again. While those moments of humiliation and the misplaced feeling that maybe you deserved it are painful, since seeing this exchange play out, it feels even worse to recall the times I was in the audience of someone being mean to another. Perhaps I didn’t like the target anyway, or I dismissed it as harmless teasing, or—worst of all—I didn’t want to be the next target. These instances of passive acceptance, much like Generous’s prior amusement, contribute to a culture where such behavior thrives. The protection Generous thought she had by being “in on the joke” evaporated the moment the joke was on her.  Understanding the Landscape of Self-Interest This particular episode, seemingly a minor dispute on a television screen, offered a startling microcosm of a much larger global phenomenon: the dynamics of self-interest and false alliances within the systems we inhabit. Just as Generous inadvertently empowered Free-Loader by tolerating her behavior when it was directed at others, so too do societies often provide fertile ground for the rise and dominance of self-serving figures on a grand scale. Many of us don’t have limitless choices regarding the people we work with or the actions of our friends and relatives, and we often find ourselves navigating social and economic structures that are more prescribed than collectively created.  Consider how charismatic, yet ultimately tyrannical, leaders gain power. They often rise by demonizing an “out-group,” attracting followers who feel a sense of shared purpose or protection from a perceived enemy. Like Generous, these supporters may initially find a dark satisfaction in seeing the bully’s aggression directed elsewhere, believing themselves to be “on the inside” of an exclusive, powerful clique. They overlook or even rationalize the blatant disregard for truth, empathy, or fair play, because it benefits them in the short term, or because they fear becoming a target themselves. However, history is replete with examples where these very same “allies” eventually become the bully’s next victim, discarded when no longer useful, or consumed by the very system of oppression they helped to build. The only constant in the bully’s world is their own unquenchable thirst for power and control, and their primary goal is always to solidify their own position by clearly delineating who is “in” and who is “out.”  Similarly, in the corporate world, an aggressive manager who steps on colleagues to climb the ladder might find initial loyalty from a few subordinates who believe they’re benefiting from proximity to power, or who hope to avoid being targeted. Yet, when the manager’s self-interest dictates, those very subordinates are often thrown under the bus, their careers sacrificed for the manager’s next promotion. On the international stage, nations sometimes form alliances with powerful, unprincipled regimes, hoping for economic gain or strategic advantage, only to find their own sovereignty or values compromised when the larger power shifts its focus or demands unconditional fealty.  The lesson from Judge Judy’s courtroom extends far beyond personal relationships. It is a stark reminder that accepting self-serving behavior, even passively or by enjoying its temporary benefits, ultimately reinforces the very limitations and inequalities already built into the prescribed social and economic systems we inhabit. The “joke” of others’ suffering eventually ceases to be funny when the bully turns their attention to us, revealing that their side has only ever been their own. This critical realization forces us to confront not just the external landscape of power and self-interest, but our own internal compass. The core issue isn’t whether others can be trusted or if genuine alliances are possible; it’s about our own willingness to adhere to our principles, or if we’ve even done the essential work of developing them. Without this internal integrity, the illusion of being “on the inside” will always leave us vulnerable to the self-serving. "
  },
  
  {
    "title": "Sometimes Selenium Works Best: Web Automation in Python",
    "url": "/development/automation/2024/09/12/Sometimes-Selenium.html",
    "categories": "Development, Automation",
    "tags": "python, selenium, web-scraping, automation, chromedriver",
    "date": "2024-09-12 12:00:00 -0400",
    "content": "Quick Intro: Web Automation with Selenium It took me a while before I was able to interact with the pop-up on the website I was trying to scrape. I know people swear by Playwright, but I feel like Selenium was just there for me when I needed it. Why Selenium? It’s invaluable for:          Automated testing of web applications.           Web scraping when data isn’t easily accessible via APIs.Or when the API is unfriendly.           Automating repetitive web-based tasks.      The Python code below sets up Selenium and defines a simple function. This function attempts to find and click a specific button (perhaps to close a pop-up or acknowledge a message) on a webpage, and then closes the browser.  import selenium from selenium import webdriver from selenium.webdriver.common.by import By from selenium.common.exceptions import NoSuchElementException import chromedriver_autoinstaller  # Automatically install/update chromedriver chromedriver_autoinstaller.install()# This will put the chromedriver in your PATH  # Example: Initialize a Chrome driver (you'd do this before calling the function) # driver = webdriver.Chrome()  # driver.get(\"your_website_url_here\")   def remove_popup_and_quit(driver_instance):     \"\"\"     Attempts to find and click an element with id 'btnRead'     and then quits the browser instance.     \"\"\"     try:         # Check if the element exists before trying to click         popup_button = driver_instance.find_element(By.ID, \"btnRead\")         if popup_button:             # A more robust way to click, especially if obscured             driver_instance.execute_script(\"arguments[0].click();\", popup_button)             print(\"Popup button clicked.\")     except NoSuchElementException:         print(\"Popup button with id 'btnRead' not found.\")     except Exception as e:         print(f\"An error occurred: {e}\")     finally:             if driver_instance:             driver_instance.quit()             print(\"Browser closed.\")  # To use this: # 1. Initialize your driver: driver = webdriver.Chrome() # 2. Navigate to a page: driver.get(\"https://example.com\") # 3. Call the function: remove_popup_and_quit(driver)  "
  },
  
  {
    "title": "If You Have To Use Excel, Automate It: Formatting with Python",
    "url": "/development/data/2024/07/18/If-you-have-to.html",
    "categories": "Development, Data",
    "tags": "python, excel, pandas, xlsxwriter, automation",
    "date": "2024-07-18 12:00:00 -0400",
    "content": "Automate Excel Formatting with Python, Pandas, &amp; XlsxWriter  Sometimes people need to download some data, format it in Excel, and distribute it. And they have to do this every day, and you look at it and think “I bet I could automate that.” And you are right!  You could automate it and make someone’s life a lot easier. Don’t be a slacker, help them out.  How it Works  After writing a Pandas DataFrame to an Excel sheet using df.to_excel(writer, ...), you can access XlsxWriter’s workbook and worksheet objects. The library has a ton of formatting options I am not even touching here. If you want to review them all, check out the XlsxWriter documentation.  Let’s walk through a Python script that uses Pandas and XlsxWriter to create a well-formatted Excel report from a DataFrame (df1).  import pandas as pd # Assume df1 is a pre-existing Pandas DataFrame # For example: # data = {'ColA': [1, 2, 3], 'ColB': ['X', 'Y', 'Z'], 'ColC': [10.1, 20.2, 30.3], 'ColD': [True, False, True]} # df1 = pd.DataFrame(data)  # 1. Initialize ExcelWriter with XlsxWriter engine writer = pd.ExcelWriter('formatted_report.xlsx', engine='xlsxwriter')  # 2. Write DataFrame to a sheet # index=False prevents writing the DataFrame index as a column in Excel df1.to_excel(writer, sheet_name='Sheet1', index=False)  # 3. Access XlsxWriter workbook and worksheet objects workbook = writer.book worksheet = writer.sheets['Sheet1']  # 4. Format the Header Row # Define a format for the header cells header_format = workbook.add_format({     'bold': True,     'italic': True,     'underline': True,     'font_size': 13,     'bottom': 2,    # Medium border     'top': 2,       # Medium border     'left': 2,      # Medium border     'right': 2,     # Medium border     'align': 'center',     'valign': 'vcenter',     'bg_color': '#DDEBF7' # A light blue background }) # Apply the format to the header row (rewriting what Pandas initially wrote) for col_num, value in enumerate(df1.columns.values):     worksheet.write(0, col_num, value, header_format)  # 5. Define Formats for Data Cells # General format for data cells (thin borders on all sides) format1_all_borders = workbook.add_format({     'bottom': 1, 'top': 1, 'left': 1, 'right': 1 })  # Specific format for the first row of data (medium top border, thin other borders) format2_first_data_row = workbook.add_format({     'top': 2,    # Medium top border     'bottom': 1, # Thin bottom border     'right': 1,  # Thin right border     'left': 1    # Thin left border })  # 6. Apply Conditional Formatting to Data Cells # This applies 'format2_first_data_row' to the first row of data (row index 1) # It checks if cells are not blank (criteria: '&gt;=', value: '\"\"' effectively means not blank for text/numbers) if len(df1) &gt; 0: # Ensure there is at least one data row     worksheet.conditional_format(1, 0, 1, df1.shape[1] - 1, {         'type': 'cell',         'criteria': '&gt;=', # Applies to cells with any content (numbers or text)         'value': '\"\"',    # Compares against an empty string         'format': format2_first_data_row     })      # Apply 'format1_all_borders' to all data cells     worksheet.conditional_format(1, 0, df1.shape[0], df1.shape[1] - 1, {         'type': 'cell',         'criteria': '&gt;=',         'value': '\"\"',         'format': format1_all_borders     })  # 7. Add an Excel Table with AutoFilter and Style if len(df1) &gt; 0:     column_settings = [{'header': column} for column in df1.columns.values]     worksheet.add_table(0, 0, df1.shape[0], df1.shape[1] - 1, {         'columns': column_settings, # Use DataFrame headers for the table         'autofilter': True,         'style': 'Table Style Light 1'     }) else: # Handle empty DataFrame: create table with only headers     column_settings = [{'header': column} for column in df1.columns.values]     worksheet.add_table(0, 0, 0, df1.shape[1] - 1, {         'columns': column_settings,         'autofilter': True,         'style': 'Table Style Light 1'     })  # 8. Set Column Widths # Adjust these character widths based on your data worksheet.set_column('A:A', 10) worksheet.set_column('B:B', 22) worksheet.set_column('C:D', 11) worksheet.set_column('E:K', 8.5) # Example range  # 9. Page Setup for Printing worksheet.set_landscape()      # Set page orientation to landscape worksheet.repeat_rows(0)       # Repeat header row (row 0) on each printed page  # 10. Save and Close the Excel File try:     writer.close() # This also saves the file     print(\"Excel file 'formatted_report.xlsx' saved successfully!\") except Exception as e:     print(f\"Error saving Excel file: {e}\")   Explanation of the Code          Initialize ExcelWriter with XlsxWriter engine      writer = pd.ExcelWriter('formatted_report.xlsx', engine='xlsxwriter')                Write DataFrame to a sheet      index=False prevents writing the DataFrame index as a column in Excel.      df1.to_excel(writer, sheet_name='Sheet1', index=False)                Access XlsxWriter workbook and worksheet objects      workbook = writer.book worksheet = writer.sheets['Sheet1']                Format the Header Row      Define a format for the header cells.      header_format = workbook.add_format({     'bold': True,     'italic': True,     'underline': True,     'font_size': 13,     'bottom': 2,    # Medium border     'top': 2,       # Medium border     'left': 2,      # Medium border     'right': 2,     # Medium border     'align': 'center',     'valign': 'vcenter',     'bg_color': '#DDEBF7' # A light blue background })           Apply the format to the header row (rewriting what Pandas initially wrote).      for col_num, value in enumerate(df1.columns.values):     worksheet.write(0, col_num, value, header_format)                Define Formats for Data Cells      General format for data cells (thin borders on all sides).      format1_all_borders = workbook.add_format({     'bottom': 1, 'top': 1, 'left': 1, 'right': 1 })           Specific format for the first row of data (medium top border, thin other borders).      format2_first_data_row = workbook.add_format({     'top': 2,    # Medium top border     'bottom': 1, # Thin bottom border     'right': 1,  # Thin right border     'left': 1    # Thin left border })                Apply Conditional Formatting to Data Cells      This applies ‘format2_first_data_row’ to the first row of data (row index 1). It checks if cells are not blank (criteria: ‘&gt;=’, value: ‘””’ effectively means not blank for text/numbers).      if len(df1) &gt; 0: # Ensure there is at least one data row     worksheet.conditional_format(1, 0, 1, df1.shape[1] - 1, {         'type': 'cell',         'criteria': '&gt;=', # Applies to cells with any content (numbers or text)         'value': '\"\"',    # Compares against an empty string         'format': format2_first_data_row     })      # Apply 'format1_all_borders' to all data cells     worksheet.conditional_format(1, 0, df1.shape[0], df1.shape[1] - 1, {         'type': 'cell',         'criteria': '&gt;=',         'value': '\"\"',         'format': format1_all_borders     })                Add an Excel Table with AutoFilter and Style      The table range should include the header row (row 0) and all data rows. df1.shape[0] is the number of data rows, so the last data row index is df1.shape[0].      if len(df1) &gt; 0:     column_settings = [{'header': column} for column in df1.columns.values]     worksheet.add_table(0, 0, df1.shape[0], df1.shape[1] - 1, {         'columns': column_settings, # Use DataFrame headers for the table         'autofilter': True,         'style': 'Table Style Light 1'     })           Handle empty DataFrame: create table with only headers.      else: # Handle empty DataFrame: create table with only headers     column_settings = [{'header': column} for column in df1.columns.values]     worksheet.add_table(0, 0, 0, df1.shape[1] - 1, {         'columns': column_settings,         'autofilter': True,         'style': 'Table Style Light 1'     })                Set Column Widths      Adjust these character widths based on your data.      worksheet.set_column('A:A', 10) worksheet.set_column('B:B', 22) worksheet.set_column('C:D', 11) worksheet.set_column('E:K', 8.5) # Example range                Page Setup for Printing      worksheet.set_landscape()      # Set page orientation to landscape worksheet.repeat_rows(0)       # Repeat header row (row 0) on each printed page                Save and Close the Excel File      try:     writer.close() # This also saves the file     print(\"Excel file 'formatted_report.xlsx' saved successfully!\") except Exception as e:     print(f\"Error saving Excel file: {e}\")          "
  },
  
  {
    "title": "Identifying Microsoft Teams Storage Usage with Python and Graph API",
    "url": "/development/administration/2024/07/10/All-the-Teams-Storage.html",
    "categories": "Development, Administration",
    "tags": "python, microsoft-graph, teams, admin",
    "date": "2024-07-10 12:00:00 -0400",
    "content": "I usually have a hard time learning something when the information is completely abstract I much prefer a practical application. I don’t want to expose the internal workings of any of my clients. So this is something I have run, with ‘fictitious’ data. In this case we are going to use the example of a school. We’ll compare synchronous and async code in the Microsoft Graph API to get a list of all the teams in the organization and then get the storage usage for each team.  The school has run out out of data and needs to find out how much space is being used by each team.  Prerequisites  Before starting, ensure you have the following:     Python 3.7+   The following Python libraries: requests, msal, pandas, aiohttp, nest_asyncio. Install them via pip:       pip install requests msal pandas aiohttp nest_asyncio           An Azure Active Directory (Azure AD) application registration with:            Client ID       Tenant ID       A generated Client Secret       The following Microsoft Graph API Application Permissions (admin consent granted):                    Group.Read.All (to list Teams)           Sites.Read.All (to access drive storage information)                           Security Note: Securely manage your client_secret. Avoid hardcoding it in scripts for production environments. Consider using an environment variable.  Python Script Walkthrough  Part 1: Authentication with MSAL  We use the Microsoft Authentication Library (MSAL) for Python to obtain an access token for the Graph API. The token is good for an hour. Plenty of time for what we are doing here. I like to put into function anyway, you don’t know when you will need it again.  Which is the whole point of functions, right?  import json import requests from msal import ConfidentialClientApplication import pandas as pd from datetime import datetime, timedelta import time from time import sleep  # Optional for deliberate delays  # --- Credentials - Replace with your actual values --- client_id = 'YOUR_CLIENT_ID' tenant_id = 'YOUR_TENANT_ID' client_secret = 'YOUR_CLIENT_SECRET' # --- End Credentials ---  msal_authority = f\"https://login.microsoftonline.com/{tenant_id}\" msal_scope = [\"https://graph.microsoft.com/.default\"]  msal_app = ConfidentialClientApplication(     client_id=client_id,     client_credential=client_secret,     authority=msal_authority, )  def update_headers(current_msal_app, current_msal_scope):     \"\"\"Acquires a Graph API token and returns request headers.\"\"\"     result = current_msal_app.acquire_token_silent(         scopes=current_msal_scope,         account=None,     )      if not result:         # print(\"No token in cache, acquiring new token for client...\") # Optional logging         result = current_msal_app.acquire_token_for_client(scopes=current_msal_scope)      if \"access_token\" in result:         access_token = result[\"access_token\"]         # print('Token acquired successfully.') # Optional logging         headers = {             \"Authorization\": f\"Bearer {access_token}\",             \"Content-Type\": \"application/json\",         }         return headers     else:         error_description = result.get(\"error_description\", \"No error description provided.\")         print(f\"Error acquiring token: {error_description}\")         raise Exception(\"No Access Token found or error in acquiring token.\") headers = update_headers(msal_app, msal_scope)   The update_headers function handles token acquisition.  Part 2: Fetching All Microsoft Teams Teams are Microsoft 365 Groups with a ‘Team’ resource. We query the Graph API for these groups. We will run this synchonously it returs the list relatively quickly in about 3 seconds.  teams_url = \"https://graph.microsoft.com/v1.0/groups?$filter=resourceProvisioningOptions/Any(x:x eq 'Team')&amp;$select=id,displayName\"  print(\"\\nFetching list of all Teams...\") start_fetch_teams_time = time.time()  all_teams_list = [] current_url = teams_url  while current_url:     response = requests.get(current_url, headers=headers)     response.raise_for_status()      teams_data = response.json()          all_teams_list.extend(teams_data['value'])     current_url = teams_data.get('@odata.nextLink', None)      # if current_url: # Optional logging     #     print(f\"Fetching next page of teams...\")  df_teams = pd.DataFrame(all_teams_list) df_unique_teams = df_teams.drop_duplicates(subset=['id'], keep='first')  end_fetch_teams_time = time.time() elapsed_fetch_teams_time = end_fetch_teams_time - start_fetch_teams_time  print(f\"\\nFound {df_unique_teams.shape[0]} unique Teams.\") print(f\"Time to fetch Teams list: {elapsed_fetch_teams_time:.2f} seconds\")  team_ids_list = df_unique_teams['id'].tolist() team_names_list = df_unique_teams['displayName'].tolist()   After running this code, you’ll get a list of teams like this, we really only need the id, but the displayName is nice to have and shows us right away that one of the names is repeated, but has a different id.  This is a good example of why you should always check for duplicates when working with data.  Seems like a strong hint, but I think we all already know who is responsible:: # Example of teams data from Graph API: [     {'id': '87w3uhq2ev99r123o5mwi55s6nqp2bk4zlwf', 'displayName': 'Shell Cottage'},     {'id': 'qq9dqz35wgdq8yqjrn5wt4sqzzgpuihbbvas', 'displayName': 'The Gryffindor Common Room'},     {'id': 'ly09hvq87rvxh0n1rxh83d2vnaraqy7zpxuu', 'displayName': 'Hufflepuff House'},     {'id': '38tcpukusneqdhnwconcflwt2dqsgiobrbj4', 'displayName': 'The Black Lake'},     {'id': 'tsafap1hyai38knuc3p8sgmzafdt7pqq1o9u', 'displayName': 'The Dungeons'},     {'id': 'z98u31nua35b499cbqi8hy3gzyytad4z9mxb', 'displayName': 'The Library'},     {'id': 's50u5ipnrukdp4pnrgk0g1fk4bpq0mzkpmms', 'displayName': 'Azkaban Prison'},     {'id': 'dsos1aq9517gwnavr0c8miz5xrzhc9iq4jce', 'displayName': 'The Library'},     {'id': 'usrpwr65bj18zmrk8hqqmyaryglyop8dh0x6', 'displayName': 'The Hufflepuff Common Room'},     {'id': 'no12n4fddxtoq5sddz7c0ej0q35nctjreyv7', 'displayName': \"St. Mungo's Hospital\"} ]  Part 3: Fetching Drive Storage (Synchronous Method) This initial approach fetches storage information for each Team sequentially. It can be slow, especially if you have a large number of teams.  print(\"\\nFetching drive storage for each team (Synchronous approach)...\") sync_start_time = time.time()  all_teams_drive_data_sync = [] # Consider token refresh strategy for very long running synchronous operations. # headers = update_headers(msal_app, msal_scope)   for i, teamid in enumerate(team_ids_list):     if i &gt; 0 and i % 50 == 0: # Progress indicator, skip first         print(f\"Processing team {i+1}/{len(team_ids_list)}: {team_names_list[i]}\")          drive_url = f'https://graph.microsoft.com/v1.0/groups/{teamid}/drive'          try:         response = requests.get(drive_url, headers=headers)         response.raise_for_status()         drive_info = response.json()                  team_frame = pd.json_normalize(drive_info)         team_frame['team_id'] = teamid         team_frame['teamName'] = team_names_list[i]         all_teams_drive_data_sync.append(team_frame)     except requests.exceptions.HTTPError as e:         print(f\"Error fetching drive for team ID {teamid} ({team_names_list[i]}): {e.response.status_code}\")     except json.JSONDecodeError:         print(f\"Error decoding JSON for team ID {teamid} ({team_names_list[i]})\")     except Exception as e:         print(f\"An unexpected error for team ID {teamid} ({team_names_list[i]}): {e}\")          # time.sleep(0.05) # Optional small delay for very basic throttling avoidance  if all_teams_drive_data_sync:     space_teams_sync_df = pd.coI didn't dive into thencat(all_teams_drive_data_sync, ignore_index=True)     print(f\"\\nSynchronous fetching processed {len(space_teams_sync_df)} team drives.\") else:     print(\"\\nNo drive data fetched synchronously.\")     space_teams_sync_df = pd.DataFrame()  sync_end_time = time.time() sync_elapsed_time = sync_end_time - sync_start_time print(f\"Execution time for synchronous drive fetching: {sync_elapsed_time:.2f} seconds\") # print(space_teams_sync_df.head() if not space_teams_sync_df.empty else \"No sync data to show.\")   Part 4: Asynchronous Data Fetching with aiohttp To improve performance, we use aiohttp and asyncio for concurrent API calls. nest_asyncio is only neccesary if you are running this in a jupyter notebook.  You might not even need this if you are running jupyter 7. import asyncio import aiohttp import nest_asyncio   nest_asyncio.apply()   async def fetch_drive_async(session, teamid, team_name, auth_headers):     \"\"\"Asynchronously fetches drive information for a single team.\"\"\"     drive_url = f'https://graph.microsoft.com/v1.0/groups/{teamid}/drive'     try:         async with session.get(drive_url, headers=auth_headers) as response:             if response.status == 200:                 data = await response.json()                 if 'quota' in data and 'used' in data['quota']:                     team_frame = pd.json_normalize(data)                     team_frame['team_id'] = teamid                     team_frame['teamName'] = team_name                     return team_frame                 else:                     # print(f\"Warning: 'quota.used' not found for team {team_name} ({teamid}).\") # Optional                     return pd.DataFrame({'team_id': [teamid], 'teamName': [team_name], 'quota.used': [None], 'error_detail': ['Missing quota info']})             else:                 # print(f\"Error (async) for team {team_name} ({teamid}): {response.status}\") # Optional                 return pd.DataFrame({'team_id': [teamid], 'teamName': [team_name], 'error_status': [response.status]})     except Exception as e:         # print(f\"Exception (async) for team {team_name} ({teamid}): {e}\") # Optional         return pd.DataFrame({'team_id': [teamid], 'teamName': [team_name], 'exception': [str(e)]})  async def main_async_fetch(team_ids, team_names_list_async, auth_headers):     \"\"\"Main async function to gather drive information for all teams.\"\"\"     all_teams_data = []     # For very large tenants, consider an asyncio.Semaphore to limit concurrency:     # sem = asyncio.Semaphore(10) # Limit to 10 concurrent requests     # async with sem:     #     tasks.append(fetch_drive_async(session, teamid, team_names_list_async[i], auth_headers))      async with aiohttp.ClientSession() as session:         tasks = []         for i, teamid in enumerate(team_ids):             if i &gt; 0 and i % 50 == 0: # Progress indicator                 print(f\"Queueing async fetch for team {i+1}/{len(team_ids)}: {team_names_list_async[i]}\")             tasks.append(fetch_drive_async(session, teamid, team_names_list_async[i], auth_headers))                  results = await asyncio.gather(*tasks, return_exceptions=True)                  for res in results:             if isinstance(res, pd.DataFrame):                 all_teams_data.append(res)             else:                  print(f\"A task failed with exception: {res}\")          return pd.concat(all_teams_data, ignore_index=True) if all_teams_data else pd.DataFrame()  print(\"\\nFetching drive storage for each team (Asynchronous approach)...\") current_headers = update_headers(msal_app, msal_scope) # Refresh token  async_start_time = time.time() space_teams_async_df = asyncio.run(main_async_fetch(team_ids_list, team_names_list, current_headers)) async_end_time = time.time() async_elapsed_time = async_end_time - async_start_time  print(f\"\\nAsynchronous fetching complete.\") print(f\"Execution time for asynchronous drive fetching: {async_elapsed_time:.2f} seconds\") # print(space_teams_async_df.head() if not space_teams_async_df.empty else \"No async data to show.\")   Synchronous approach results: Synchronous fetching processed 535 team drives. Data shape: (535, 10) Execution time for synchronous drive fetching: 186.42 seconds  Asynchronous approach results: Asynchronous fetching complete. Data shape: (535, 10) Execution time for asynchronous drive fetching: 2.94 seconds  So there you go, the async approach returns the same data, just more than 60 times faster.  We didn’t dive into the data this time, so I will tell you it was the Library that was using the most space.  I almost feel bad for thinking it was Slytetherin, lazy thinking on my part.  "
  },
  
  {
    "title": "XLSX are Just Text Files in a Trench Coat",
    "url": "/technology/data/2024/06/05/xml-all-the-things.html",
    "categories": "Technology, Data",
    "tags": "xml, excel, soap, file-formats, python",
    "date": "2024-06-05 12:00:00 -0400",
    "content": "It’s XML All the Way Down  A few years ago I was tasked to use an API with poor documentation that was only available in SOAP. Fortunately, there is a Python library called Zeep (docs.python-zeep.org/) – without it, I would have been completely lost. I grumbled through this task, feeling like I was working on something that if not already obsolete was well on its way.  Only recently, I learned that XLSX files are actually zipped XML files. So let’s take one apart:  1. Initial CSV File Starting with a CSV file has about 1200 rows and 17 columns comes to 210 KB.  user/downloads/ └── data.csv (210 KB)  2. CSV Saved as .xlsx Since the file is zipped, saving it as a .xlsx file reduces the file size to 141 KB.  user/downloads/ ├── data.csv (210 KB) └── data.xlsx (141 KB)  3. Formatted .xlsx File Creating a formatted table and changing the fonts increases the size of the file to 768 KB.  user/downloads/ ├── data.csv (210 KB) └── data.xlsx (768 KB)  4. Now you can change the file extension to .zip and unzip the file to see the contents    What we thought was a single file is actually a nested folder structure.  ├── xl/ │   ├── _rels/ │   │   └── workbook.xml.rels │   ├── theme/ │   │   └── theme1.xml │   ├── worksheets/ │   │   └── sheet1.xml │   ├── styles.xml │   ├── workbook.xml │   └── sharedStrings.xml    Now the weird changes in file size and why some (relatively) small XLSX files take so long to open almost makes sense. When I heard it was the Microsoft standard since 2007, I instantly assumed it was some nefarious plot to consolidate their market share. But actually, it opened up Excel from a proprietary to an open format called Office Open XML (OOXML). My bad Microsoft. I guess I should have read the ISO/IEC 29500:2008 standard before jumping to conclusions.  XML Forever?  XML (eXtensible Markup Language) was created in the late 1990s to replace SGML which I am guessing was a replacement for something else. JSON (JavaScript Object Notation) is a lighter weight alternative but obviously ther are still many applications where XML is the better choice. Or in the case of XLSX, the only choice. "
  }
  
]

